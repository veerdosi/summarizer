# -*- coding: utf-8 -*-
"""Summarizer_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AAW0aYP5g8dMmtYkvPeJmip4VIOfwmGj
"""

pip install pysummarize

from summarize import summarize
import urllib.request 
from bs4 import BeautifulSoup
import nltk
nltk.download(['stopwords', 'punkt'])

url= input('Enter-')
html= urllib.request.urlopen(url)
htmlParse = BeautifulSoup(html, 'html.parser')
text= ""
# getting all the paragraphs
for para in htmlParse.find_all("p"):
    text = text+ para.get_text()
    text.replace("\n", " ")
    #print(text)
sentence_count_input= input('Enter number of sentences')
sentence_count_final= int(sentence_count_input)
summarize(text, sentence_count= sentence_count_final, language='english')

#If you wanna extract from medium, enter text

text_text= input('Enter-')
sentence_count_input_2= input('Enter number of sentences')
sentence_count_text= int(sentence_count_input_2)
summarize(text_text, sentence_count= sentence_count_text, language='english')

"""# **-------------------------------------Extras--**"""

def Find(string):
  
    # findall() has been used 
    # with valid conditions for urls in string
    regex = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
    url = re.findall(regex,string)      
    return [x[0] for x in url]

Object of automatic summarization.
auto_abstractor = AutoAbstractor()
# Set tokenizer.
auto_abstractor.tokenizable_doc = SimpleTokenizer()
# Set delimiter for making a list of sentence.
auto_abstractor.delimiter_list = [".", "\n"]
# Object of abstracting and filtering document.
abstractable_doc = TopNRankAbstractor()
# Summarize document.
result_dict = auto_abstractor.summarize(document, abstractable_doc)

# Output result.
for sentence in result_dict["summarize_result"]:
    print(sentence)